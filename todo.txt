;;;;;;;;;;;;;;;; Implement


  Auto-merging above and beyond what's given.  (based on same opt-sets &
set of plans to go.  Store with each node, minimal rep. of plan-to-go??)
  ; subsumed by above proposal -- maybe ?

  ALGs should have some mode for finding suboptimal plans.
 - Is there a simple Weighted A*/AHSS-like algorithm for ALGs, and what does it look like? Specifically, it seems like extracting optimistic state sequences may be the wrong thing to do here (instead, bias things to include more pessimistic states in the sequences?).
 TODO: a way to implement extract-a-solution (or refine potentially suboptimal things...)

  Split-set for ALG

  If we make things look worse in early part of the graph, we have to
touch *everything* to fix it.  Partial way around this by keeping
track of "delta"?


;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; Minor

 - Enforce consistency, add assertions back to sbp

 - Cache best child

 - Going backwards, keep track of the reward of the second-best path. 

 - After refining a node, instead of returning directly to the root, progress forward along the optimal state sequence.  If at any point the progressed valuation assigns the regressed state a reward corresponding to a plan at least as good as the second-best path, call backwards-pass starting at this node.

 - At the other end, cache the max-gap for the current state sequence going forward from the root.  If, during a backwards pass, you hit the cached state sequence and the current max-gap is > the cached one, stop going backwards and refine right away.

 - Eliminating bad plans from ALG sections
    - Bad suffixes with splitting
    - Dead plan sections (-Inf opt or no refinements)
   - Dominated sections ??
    - Ancestor-dominated sections ? ?
TODO TODO: kill off dead-end fragments.  Just don't return them.
  (better than making next-map have weak val refs).




;;;;;;;;;;;;;;;; Think About

 Incremental expansion.

 How to use pessimistic descriptions in ALG

hierarchical preconditions -> bhaskara  (lots more spurious states).

- What's the relationship, if any, between optimal ALG search and
  algorithms like AO*?  

  If we keep hierarchical structure, we can sometimes save by
  using the higher-level descriptions that are removed right now.


;;;;;;;;;;;;;;;; General TODO


;;;;;;;;;;;;;;;;; Progression/subsumption efficiency

 - Incremental progression/merging.  It should be possible to progress only "diffs" going forward, since usually only small changes will have been made to the valuations.  This will probably be very important if the valuations get very large.  Similar indexing may allow more efficiently finding the best precursor (for OR-nodes) and the best regressed state (for AND-nodes).

Incremental progression v.2. (only progress best clauses first...) -- could
allow more merging without degenerating to symbolic BFS.

Clauses record provenance.  Assume optimsitic consistency.  rewards
always go down.  Can only affect some subest of progressed results.
By decreasing reward by x for decreased by x, or by anything for
killed state.

(If doing fancy valuation tricks, much watch out for
identitydescrpition, conditional, ...)

Might as well store provenance too, for regression ...
... and implement incremental progression/merging?
 -- But problem with provenance is things may be OK even if 
    valuation is out of date, in which case chain is invalid
    ... . . . . ...?

; Smarter way to compare vectors of rewards in subsumption ...


Better to regress partial clauses ...




---------------------------------------------------------


Later

Fancier PDF display ?

Ideas on probabilistic descriptions, synthesis, starting with flat
hierarchy, learning descriptions, macros, hierarchies bottom up.
Useful HLAs are ones with high success probability, low computational
complexity.   All of these things are context-sensitive, can be
imprecise, except for solid opt/pess descriptions.  

TODO: figure out what to do with forall conditions in NCSTRIPS.

